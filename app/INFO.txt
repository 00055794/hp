Overview
Goal: Predict Kazakhstan apartment prices in real KZT (not ln) with a simple web app that supports form input, batch uploads (CSV/XLSX), and shows clickable points on a Kazakhstan map.
Core approach: A neural network trained on features from the listing + regional socio-economic context + a geographic segmentation code. Predictions are computed in ln-scale during training and converted to real KZT at inference.
App behavior: Uses the NN by default; falls back to a baseline model if NN artifacts aren’t present. It enriches each input with region info and displays predicted prices on a Folium map.
Inputs and Features
Required per listing (app expects these exact headers):

ROOMS, TOTAL_AREA, FLOOR, TOTAL_FLOORS, FURNITURE, CONDITION, CEILING, MATERIAL, YEAR, LATITUDE, LONGITUDE
Contextual features (added during enrichment/segmentation):

segment_code (int; geographic segment assignment)
srednmes_zarplata (average monthly salary)
chislennost_naseleniya_092025 (population)
Optional derived regional features (house_point_count, area_km2, density_hp_per_km2, composite_score) used in segmentation.
Column normalization:

TOTAL AREA is normalized to TOTAL_AREA
Latitude/Longitude are normalized to LATITUDE/LONGITUDE
Preprocessing
From notebooks and scripts (ported into the codebase):

Target: PRICE_ln = ln(PRICE). The model learns to predict ln(PRICE).
Numeric coercion: Numeric fields are coerced to numeric types.
Categorical encoding:
For NN: FURNITURE, CONDITION, MATERIAL are mapped to stable integer codes (saved in cat_mappings.json).
For baseline: OneHotEncoder is used inside a scikit-learn ColumnTransformer.
Feature selection for NN:
Base: numeric features + encoded categoricals.
If segment_code is present, LATITUDE and LONGITUDE are dropped to avoid double-counting location (mirrors the notebook’s intent).
Scaling:
NN uses MinMaxScaler for inputs and ln-target.
Baseline uses StandardScaler for numeric columns inside the pipeline.
Segmentation and Regional Enrichment
Two layers, modeled after the notebook.

Regional enrichment (from Stat_KZ092025.xlsx)
Build a centroids GeoDataFrame of regional stats.
Robustly assign stats to regions:
Normalize CRS and snap centroids that fall just outside polygons.
Primary spatial join (within).
Fallback A: name/code join if a shared key exists.
Fallback B: nearest polygon assignment for any remaining unmatched centroids.
Aggregate centroid attributes to polygons (mean).
IDW fill: For polygons with missing attributes, use inverse distance weighting from k nearest centroids to fill gaps.
Derived region metrics:
house_point_count (if points were provided), area_km2, density_hp_per_km2,
composite_score: standardized socio-economic features combined with a geometric decay weight (alpha ≈ 0.85).
Fine-grained housing segmentation (heuristic)
Assign each listing (hp_gdf) to its region (spatial join).
Compute a heuristic target number of segments:
max(k_factor * sqrt(N), N / desired_avg_points), with region-wise allocation weighted by n^alpha (more points → more segments).
Cluster per region with MiniBatchKMeans to get micro-segments (segment_local).
Optional second pass: Split overly large segments again to achieve desired sizes.
Polygonization:
For each segment, create a buffered hull of member points, clipped to region geometry.
Greedy non-overlap enforcement to remove polygon intersections.
Outputs:
Segment polygons as GeoJSON,
Points with segment_id,
Segment summary by size and basic stats,
segment_code_map.json: a stable mapping from segment_id (string) to segment_code (int), akin to LabelEncoder on sorted unique values.
Runtime assignment

At app inference, segment_code is assigned by one of:
Spatial join with the generated polygons + mapping (preferred),
Or fallback: nearest regional centroid (if polygons are absent).
The app stores segment_code as an integer for the model.
Model Definition
Two models exist; NN is the preferred path.

Baseline (fallback)

Pipeline: ColumnTransformer (StandardScaler for numeric, OneHotEncoder for categoricals) + RandomForestRegressor trained on PRICE_ln.
Inference: predict ln(price) and convert to KZT via exp().
Neural Network (primary)

Architecture: 64 → 16 → 1 (ReLU activations; final Identity for regression).
Inputs: numeric features + encoded categoricals; optional segment_code; regional context if present. LAT/LON dropped when segment_code is available.
Scaling: MinMaxScaler for X and for ln-target y.
Training: 80/20 train/validation split, Adam optimizer, MSE loss, ReduceLROnPlateau LR scheduler, early stopping on validation loss.
Artifacts:
model.pt (weights), scaler_X.joblib, scaler_y.joblib,
feature_list.json (exact input order),
cat_mappings.json (stable categorical maps).
Inference: transform inputs using scaler_X, predict ln(price), inverse-transform with scaler_y, then exp() → real KZT.
App Flow
Single or batch data ingestion (CSV/XLSX with the required headers).
Enrichment:
Assign region stats and segment_code (via polygons + map or nearest centroid).
Prediction:
Prefer NN if artifacts exist; else fallback to baseline pipeline.
Convert ln-prediction to real KZT.
Visualization:
Folium map centered on Kazakhstan or mean coordinates of uploaded points.
Circle markers; tooltip shows predicted KZT price; popup shows user-supplied features + predicted price.
Persistence:
Predictions (single/batch) persist in session so they don’t disappear on re-rerun.
Downloads:
Predictions as CSV/XLSX.
Templates for easy batch creation.
Artifacts and Files (102025 and 102025/app)
Segmentation/Enrichment:
regions_enriched.shp (regions with socio-economic attributes),
segments_fine_heuristic_polygons.geojson,
segments_fine_heuristic_points.parquet,
segments_fine_heuristic_summary.csv,
segment_code_map.json (stable mapping).
NN Training/Inference:
nn_model/ (model.pt, scalers, feature list, categorical mappings),
train_nn.py, nn_inference.py.
App/UI:
app.py (Streamlit app), predictor.py (runtime enrichment + helpers),
requirements.txt,
template.csv, sample_input.xlsx,
.streamlit/config.toml (dark theme).